{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADUP Trained Model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./(Preprocessed)Emotion_classify_Data(Labeled).csv')\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=dataset['Emotion'].nunique())\n",
    "\n",
    "# Fine-tune BERT on your dataset\n",
    "learning_rate = 2e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels = dataset['Emotion']\n",
    "\n",
    "# Instantiate the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder to the encoded emotion labels\n",
    "label_encoder.fit(emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer states loaded successfully.\n",
      "Epoch: 7, Best Validation Loss: 0.09719424509431089, Early Stop Count: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Programming\\python\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('last_trained_model_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Load model and optimizer states with strict=False to ignore missing keys\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Update other necessary variables\n",
    "epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "early_stop_count = checkpoint['early_stop_count']\n",
    "\n",
    "print(f\"Model and optimizer states loaded successfully.\")\n",
    "print(f\"Epoch: {epoch}, Best Validation Loss: {best_val_loss}, Early Stop Count: {early_stop_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: joy\n",
      "Predicted Label: tensor([[-2.6676, -2.7545,  6.4300]])\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(text, label_encoder):\n",
    "    # Tokenize the text using the same tokenizer used during training\n",
    "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)  # Move input to the same device as the model\n",
    "    attention_mask = inputs['attention_mask'].to(device)  # Move attention mask to the same device as the model\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Decode the predicted label using label_encoder\n",
    "    predicted_emotion = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "    return predicted_emotion, logits\n",
    "\n",
    "# Example usage\n",
    "text = \"Super happy today\"\n",
    "predicted_emotion, x = predict_emotion(text, label_encoder)\n",
    "print(f'Predicted Emotion: {predicted_emotion}')\n",
    "print(f'Predicted Label: {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't cry because it's over, smile because it ...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Be yourself; everyone else is already taken.</td>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two things are infinite: the universe and huma...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Be who you are and say what you feel, because ...</td>\n",
       "      <td>Bernard M. Baruch</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36932</th>\n",
       "      <td>In Buddhism, they say attachment to anything o...</td>\n",
       "      <td>Jason Mraz</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36933</th>\n",
       "      <td>I love British humor. It's just so - surreal.</td>\n",
       "      <td>Beck</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36934</th>\n",
       "      <td>I've got a sense of humor. I'm a funny guy.</td>\n",
       "      <td>Daryl Hall</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36935</th>\n",
       "      <td>Humor is such a wonderful thing, helping you r...</td>\n",
       "      <td>Lynda Barry</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36936</th>\n",
       "      <td>Life Is Full of Obstacles, Stumble Upon !!</td>\n",
       "      <td>Pratik Shelar</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36937 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Quote             Author  \\\n",
       "0      Don't cry because it's over, smile because it ...          Dr. Seuss   \n",
       "1      I'm selfish, impatient and a little insecure. ...     Marilyn Monroe   \n",
       "2           Be yourself; everyone else is already taken.        Oscar Wilde   \n",
       "3      Two things are infinite: the universe and huma...    Albert Einstein   \n",
       "4      Be who you are and say what you feel, because ...  Bernard M. Baruch   \n",
       "...                                                  ...                ...   \n",
       "36932  In Buddhism, they say attachment to anything o...         Jason Mraz   \n",
       "36933      I love British humor. It's just so - surreal.               Beck   \n",
       "36934        I've got a sense of humor. I'm a funny guy.         Daryl Hall   \n",
       "36935  Humor is such a wonderful thing, helping you r...        Lynda Barry   \n",
       "36936         Life Is Full of Obstacles, Stumble Upon !!      Pratik Shelar   \n",
       "\n",
       "      emotion  \n",
       "0         joy  \n",
       "1       anger  \n",
       "2       anger  \n",
       "3         joy  \n",
       "4         joy  \n",
       "...       ...  \n",
       "36932     joy  \n",
       "36933    fear  \n",
       "36934     joy  \n",
       "36935     joy  \n",
       "36936    fear  \n",
       "\n",
       "[36937 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotesDF = pd.read_csv('./(Preprocessed)quotes.csv')\n",
    "quotesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar_quote(input_sentence, quotes_df, input_emotion, emotion_weight=1.1):\n",
    "    # Combine all quotes into a list\n",
    "    quotes = quotes_df['Quote'].tolist()\n",
    "    authors = quotes_df['Author'].tolist()\n",
    "    emotions = quotes_df['emotion'].tolist()\n",
    "\n",
    "    # Initialize the vectorizer and fit it on all quotes\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(quotes)\n",
    "\n",
    "    # Vectorize the input sentence\n",
    "    input_vec = vectorizer.transform([input_sentence])\n",
    "\n",
    "    # Calculate cosine similarity between the input sentence and all quotes\n",
    "    cosine_similarities = cosine_similarity(input_vec, tfidf_matrix).flatten()\n",
    "\n",
    "    # Apply emotion-based weighting\n",
    "    emotion_weights = np.array([emotion_weight if emotion == input_emotion else 1.0 for emotion in emotions])\n",
    "    weighted_similarities = cosine_similarities * emotion_weights\n",
    "\n",
    "     # Get the indices of the top 10 quotes with the highest similarity scores\n",
    "    top_10_indices = np.argsort(weighted_similarities)[-5:]\n",
    "\n",
    "    # Randomly select one quote from the top 10\n",
    "    selected_index = random.choice(top_10_indices)\n",
    "\n",
    "    # Return the most similar quote and its author\n",
    "    return quotes[selected_index], authors[selected_index], emotions[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:  What should I do with my life?\n",
      "Detected emotion: anger\n",
      "Emotion score: tensor([[ 1.2938,  0.3162, -1.3738]])\n",
      "Most similar quote: Don't let what you cannot do interfere with what you can do.\n",
      "Author:  John Wooden\n",
      "Quotes Emotion:  anger\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"What should I do with my life?\"\n",
    "detected_emotion, emotion_score = predict_emotion(input_sentence, label_encoder)\n",
    "most_similar_quote, author, emotionQuotes = find_most_similar_quote(input_sentence, quotesDF, detected_emotion)\n",
    "print('Input sentence: ', input_sentence)\n",
    "print(\"Detected emotion:\", detected_emotion)\n",
    "print(\"Emotion score:\", emotion_score)\n",
    "print(\"Most similar quote:\", most_similar_quote)\n",
    "print(\"Author: \", author)\n",
    "print(\"Quotes Emotion: \", emotionQuotes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
